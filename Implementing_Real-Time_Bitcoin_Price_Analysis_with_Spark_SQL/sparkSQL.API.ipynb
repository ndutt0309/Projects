{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183c2248-ea3d-43ba-b87e-d821bba1bbc6",
   "metadata": {},
   "source": [
    "# Spark SQL API Tutorial\n",
    "\n",
    "Spark SQL is a module of the application Apache Spark, and Spark SQL facilitates working with structured data. It allows us to connect and query multiple data sources including Hive, Parquet, ORC, and more. This tutorial focuses on using Spark SQL with Python and pyspark, and showcases Spark SQL queries and functions of various complexities.\n",
    "\n",
    "Two small datasets 'books.csv' and 'prices.csv' are used to demonstrate Spark SQL functionality and to provide a brief introduction to SQL functions, statements, and clauses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265e0d58-a7cd-4edf-a0b4-96b60220e801",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:07.745839Z",
     "start_time": "2025-05-15T19:53:07.725867Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b2f997-5c9b-4238-b6d5-e5f2cea43809",
   "metadata": {},
   "source": [
    "# Getting Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1480ee9-d6a6-437d-b927-da6cbb05bdf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:07.895048Z",
     "start_time": "2025-05-15T19:53:07.747821Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import libraries.\n",
    "import logging\n",
    "import os\n",
    "import pyspark.sql\n",
    "import pyspark.sql.functions as functions\n",
    "import pyspark.sql.types as types\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9208cc9-837d-4fec-a312-9c4aa5b7648d",
   "metadata": {},
   "source": [
    "# Configuration & Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4201c94a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:12.710711Z",
     "start_time": "2025-05-15T19:53:07.897416Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/15 19:53:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/15 19:53:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#Create the Spark Session and disable Hive so that native Spark tables are created instead of tables managed by Hive.\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"API Example\") \\\n",
    "    .config(\"spark.sql.legacy.createHiveTableByDefault\", \"false\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"--add-opens java.base/java.nio=ALL-UNNAMED\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"--add-opens java.base/java.nio=ALL-UNNAMED\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7776a2a",
   "metadata": {},
   "source": [
    "# Working with the DataFrame API of Spark SQL\n",
    "\n",
    "Before we can perform any operations, we need to create a dataframe. A dataframe can be created from an existing RDD, Hive table, or Spark data sources. Here we manually create a small dataframe that contains information about a person's age and the number of books they have read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a12176b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:52.221823Z",
     "start_time": "2025-05-15T19:53:51.967995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+\n",
      "|first_name|age|books_read|\n",
      "+----------+---+----------+\n",
      "|       sue| 32|        12|\n",
      "|       eli|  3|         1|\n",
      "|       bob| 75|        24|\n",
      "|      theo| 13|         5|\n",
      "|      mary| 25|        50|\n",
      "+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframe in the Spark session.\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"sue\", 32, 12), #Each entry corresponds to one row in the dataframe.\n",
    "        (\"eli\", 3, 1),\n",
    "        (\"bob\", 75, 24),\n",
    "        (\"theo\", 13, 5),\n",
    "        (\"mary\", 25, 50)\n",
    "    ],\n",
    "    [\"first_name\", \"age\", \"books_read\"], #Column names are in order of the data added.\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484231dc",
   "metadata": {},
   "source": [
    "Now we can add a column that indicates how much of a reading fanatic each person is based on the number of books they have read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f4ca26a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:17.821790Z",
     "start_time": "2025-05-15T19:53:17.245026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|first_name|\n",
      "+----------+\n",
      "|       sue|\n",
      "|       eli|\n",
      "|       bob|\n",
      "|      theo|\n",
      "|      mary|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select and display the names of everyone in the dataframe.\n",
    "df.select(df['first_name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea27794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:18.465910Z",
     "start_time": "2025-05-15T19:53:17.826884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|first_name|(books_read + 1)|\n",
      "+----------+----------------+\n",
      "|       sue|              13|\n",
      "|       eli|               2|\n",
      "|       bob|              25|\n",
      "|      theo|               6|\n",
      "|      mary|              51|\n",
      "+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Increment the number of books read for each person and display. This does not change the dataframe, just displays the new value.\n",
    "df.select(df['first_name'], df['books_read'] + 1).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4c5221d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:19.436632Z",
     "start_time": "2025-05-15T19:53:18.468392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|avg(books_read)|\n",
      "+---------------+\n",
      "|           18.4|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compute and display the average number of books read across the dataset.\n",
    "df.select(functions.avg(\"books_read\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1364ae8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:20.137674Z",
     "start_time": "2025-05-15T19:53:19.442183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+---------------+\n",
      "|first_name|age|books_read|reading_fanatic|\n",
      "+----------+---+----------+---------------+\n",
      "|       sue| 32|        12|   intermediate|\n",
      "|       eli|  3|         1|       beginner|\n",
      "|       bob| 75|        24|   intermediate|\n",
      "|      theo| 13|         5|       beginner|\n",
      "|      mary| 25|        50|       advanced|\n",
      "+----------+---+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Use dataframe operation to insert a new column which indicates how much of a reading fanatic each person is based on the number of books they have read.\n",
    "df = df.withColumn(\n",
    "    \"reading_fanatic\",\n",
    "    functions.when(functions.col(\"books_read\") <= 10, \"beginner\")\n",
    "    .when(functions.col(\"books_read\").between(11, 30), \"intermediate\")\n",
    "    .otherwise(\"advanced\"),\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba96826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:21.058612Z",
     "start_time": "2025-05-15T19:53:20.146126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|reading_fanatic|count|\n",
      "+---------------+-----+\n",
      "|   intermediate|    2|\n",
      "|       beginner|    2|\n",
      "|       advanced|    1|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compute and display the number of books read in each reading_fanatic level.\n",
    "df.groupBy(\"reading_fanatic\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56b7d6",
   "metadata": {},
   "source": [
    "# Querying with the SQL Function\n",
    "We've run dataframe operations that are similar to simple SQL queries. Now let's use Spark Session's SQL function to run more complex queries on a larger version of our dataset from before. Additional [information](https://spark.apache.org/docs/latest/sql-data-sources-csv.html) on using CSV files for data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a48a457",
   "metadata": {},
   "source": [
    "## Load & Prep Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6a9b0",
   "metadata": {},
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a05f5d91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:56.545588Z",
     "start_time": "2025-05-15T19:53:56.398981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------------------+----------+\n",
      "|first_name|age|             genre|books_read|\n",
      "+----------+---+------------------+----------+\n",
      "|       sue| 32|            sci-fi|         8|\n",
      "|       sue| 32|         biography|         4|\n",
      "|       eli|  3|           fantasy|         1|\n",
      "|       bob| 75|           mystery|        12|\n",
      "|       bob| 75|historical fiction|         4|\n",
      "|       bob| 75|          thriller|         3|\n",
      "|       bob| 75|         biography|         5|\n",
      "|      theo| 13|            sci-fi|         2|\n",
      "|      theo| 13|           fiction|         3|\n",
      "|      mary| 25|historical fiction|         3|\n",
      "|      mary| 25|           mystery|         7|\n",
      "|      mary| 25|         dystopian|         8|\n",
      "|      mary| 25|           romance|         8|\n",
      "|      mary| 25|            satire|         4|\n",
      "|      mary| 25|            sci-fi|        14|\n",
      "|      mary| 25|            memoir|        16|\n",
      "|      john| 50|         dystopian|         8|\n",
      "|      john| 50|          thriller|        12|\n",
      "|      john| 50|        true crime|         3|\n",
      "|      jane| 61|           romance|        20|\n",
      "+----------+---+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- books_read: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load and display books dataframe from csv file and display the schema.\n",
    "books_df = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"books.csv\")\n",
    "books_df.show()\n",
    "\n",
    "books_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de3347",
   "metadata": {},
   "source": [
    "### Create a Temp View\n",
    "This view only persists as long as the Spark Session is active, once the session ends, the view is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "385f50a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:22.563014Z",
     "start_time": "2025-05-15T19:53:22.459154Z"
    }
   },
   "outputs": [],
   "source": [
    "#Remove any existing view named 'books', if already created.\n",
    "spark.catalog.dropTempView(\"books_view\")\n",
    "#Convert the dataframe to a view.\n",
    "books_df.createOrReplaceTempView(\"books_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318d3ba",
   "metadata": {},
   "source": [
    "### Create a Table\n",
    "This table is temporarily saved to Sparkâ€™s default warehouse directory. If the Spark Session ends, the data and table is lost unless the table is created in Hive Metastore. However, the directory on disk still exists, so the directory must be removed before re-creating the table. If Hive is enabled, Spark creates a permanent table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba16fb8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:54:02.073655Z",
     "start_time": "2025-05-15T19:54:01.868073Z"
    }
   },
   "outputs": [],
   "source": [
    "#If already created in current or previous SparkSession, drop table and remove the directory where the table existed.\n",
    "spark.sql(\"DROP TABLE IF EXISTS books_table\")\n",
    "shutil.rmtree(\"/data/spark-warehouse/books_table\", ignore_errors=True)\n",
    "spark.catalog.clearCache()\n",
    "#Write the books_df to a table.\n",
    "books_df.write.saveAsTable(\"books_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f51c466c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:24.140600Z",
     "start_time": "2025-05-15T19:53:24.033170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|first_name|   string|   null|\n",
      "|       age|   string|   null|\n",
      "|     genre|   string|   null|\n",
      "|books_read|   string|   null|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The 'desc' statement returns metadata or the data-definition language (ddl) of the table.\n",
    "spark.sql(\"desc books_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd607d",
   "metadata": {},
   "source": [
    "## Common Statements & Clauses\n",
    "There are numerous SQL clauses that can be a part of a query, here we explore how to implement some of the most common ones with the 'sql' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c755f12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:24.615772Z",
     "start_time": "2025-05-15T19:53:24.144791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|first_name|books_read|\n",
      "+----------+----------+\n",
      "|       sue|         8|\n",
      "|       sue|         4|\n",
      "|       eli|         1|\n",
      "|       bob|        12|\n",
      "|       bob|         4|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display the first five names and number of books read from the table using the LIMIT clause at the end of the query.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT first_name, books_read\n",
    "    FROM books_table\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88927330",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:25.160957Z",
     "start_time": "2025-05-15T19:53:24.620378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------------------+----------+\n",
      "|first_name|age|             genre|books_read|\n",
      "+----------+---+------------------+----------+\n",
      "|      jane| 61|historical fiction|        12|\n",
      "|    martin| 29|historical fiction|        10|\n",
      "+----------+---+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display rows where genre read is historical fiction and number of books read is greater than or equal to 10.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM books_table\n",
    "    WHERE genre = 'historical fiction' AND books_read >= 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54182d1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:25.883742Z",
     "start_time": "2025-05-15T19:53:25.164611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------------------+----------+\n",
      "|first_name|age|             genre|books_read|\n",
      "+----------+---+------------------+----------+\n",
      "|      alex| 19|            sci-fi|        14|\n",
      "|      ally| 42|         biography|        14|\n",
      "|      ally| 42|            memoir|         9|\n",
      "|       bob| 75|         biography|         5|\n",
      "|       bob| 75|historical fiction|         4|\n",
      "+----------+---+------------------+----------+\n",
      "\n",
      "+----------+---+---------+----------+\n",
      "|first_name|age|    genre|books_read|\n",
      "+----------+---+---------+----------+\n",
      "|      theo| 13|   sci-fi|         2|\n",
      "|      theo| 13|  fiction|         3|\n",
      "|       sue| 32|   sci-fi|         8|\n",
      "|       sue| 32|biography|         4|\n",
      "|      mary| 25|   sci-fi|        14|\n",
      "+----------+---+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display the top 5 rows from the table ordered by first_name and genre ascending.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM books_table\n",
    "    ORDER BY first_name, genre\n",
    "    LIMIT 5\n",
    "\"\"\").show()\n",
    "\n",
    "#Display the top 5 rows from the table ordered by first_name and genre descending.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM books_table\n",
    "    ORDER BY first_name DESC, genre DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdad56ce",
   "metadata": {},
   "source": [
    "## Aggregate Functions\n",
    "Many times when analyzing structured data, we want to perform aggregations on columns using certain conditions. We can achieve the same using Spark SQL with aggregate functions, GROUP BY clause, and HAVING clause. Note any non-aggregate columns in the SELECT statement must appear in GROUP BY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2257522c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:26.285685Z",
     "start_time": "2025-05-15T19:53:25.888212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------------+\n",
      "|first_name|age|total_books_read|\n",
      "+----------+---+----------------+\n",
      "|       sue| 32|              12|\n",
      "|      alex| 19|              14|\n",
      "|      josh| 22|              11|\n",
      "|       eve|  8|               7|\n",
      "|      john| 50|              23|\n",
      "|      theo| 13|               5|\n",
      "|      mary| 25|              60|\n",
      "|      jane| 61|              32|\n",
      "|    martin| 29|              14|\n",
      "|      ally| 42|              23|\n",
      "|       bob| 75|              24|\n",
      "|       eli|  3|               1|\n",
      "+----------+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find the total number of books each person read.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT first_name, age, cast(sum(books_read) as int) as total_books_read\n",
    "    FROM books_table\n",
    "    GROUP BY first_name, age\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a310f5f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:26.656925Z",
     "start_time": "2025-05-15T19:53:26.290904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+\n",
      "|             genre|total_readers|\n",
      "+------------------+-------------+\n",
      "|         biography|            3|\n",
      "|           fantasy|            2|\n",
      "|           mystery|            3|\n",
      "|           fiction|            1|\n",
      "|         dystopian|            2|\n",
      "|        true crime|            1|\n",
      "|            memoir|            2|\n",
      "|            satire|            2|\n",
      "|historical fiction|            4|\n",
      "|           romance|            2|\n",
      "|          thriller|            3|\n",
      "|            sci-fi|            5|\n",
      "+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Count how many people have read at least one book of a given genre\n",
    "spark.sql(\"\"\"\n",
    "    SELECT genre, cast(count(first_name) as int) as total_readers\n",
    "    FROM books_table\n",
    "    GROUP BY genre\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "737c9bd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:27.152581Z",
     "start_time": "2025-05-15T19:53:26.660266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+---------+\n",
      "|             genre|min_books|max_books|\n",
      "+------------------+---------+---------+\n",
      "|         biography|       14|        5|\n",
      "|         dystopian|        8|        8|\n",
      "|           fantasy|        1|        7|\n",
      "|           fiction|        3|        3|\n",
      "|historical fiction|       10|        4|\n",
      "|            memoir|       16|        9|\n",
      "|           mystery|        1|        7|\n",
      "|           romance|       20|        8|\n",
      "|            satire|        3|        4|\n",
      "|            sci-fi|       14|        8|\n",
      "|          thriller|       12|        8|\n",
      "|        true crime|        3|        3|\n",
      "+------------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Identify minimum and maximum number of books read for each genre.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT genre,\n",
    "        MIN(books_read) as min_books,\n",
    "        MAX(books_read) as max_books\n",
    "    FROM books_table\n",
    "    GROUP by genre\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4d1fe19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:27.627166Z",
     "start_time": "2025-05-15T19:53:27.155901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|    genre|avg_age|\n",
      "+---------+-------+\n",
      "|dystopian|     37|\n",
      "|   memoir|     33|\n",
      "|   satire|     23|\n",
      "|   sci-fi|     23|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display the average age of readers for each genre where genre is not fiction and the avg_age is between 10 and 40.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT genre, cast(avg(age) as int) as avg_age\n",
    "    FROM books_table\n",
    "    WHERE genre != 'fiction'\n",
    "    GROUP BY genre\n",
    "    HAVING avg(age) BETWEEN 10 and 40\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1368c9",
   "metadata": {},
   "source": [
    "## Case When Statement\n",
    "The CASE expression returns a specific value when a condition is met - consider it similar to an if-then-else statement. It can be used to create identifier columns, condition on aggreations, and more. Let's see one implementation of this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98a32e6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:27.829783Z",
     "start_time": "2025-05-15T19:53:27.632253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------------------+----------+---------------+\n",
      "|first_name|age|             genre|books_read|reading_fanatic|\n",
      "+----------+---+------------------+----------+---------------+\n",
      "|       sue| 32|            sci-fi|         8|       beginner|\n",
      "|       sue| 32|         biography|         4|       beginner|\n",
      "|       eli|  3|           fantasy|         1|       beginner|\n",
      "|       bob| 75|           mystery|        12|   intermediate|\n",
      "|       bob| 75|historical fiction|         4|       beginner|\n",
      "|       bob| 75|          thriller|         3|       beginner|\n",
      "|       bob| 75|         biography|         5|       beginner|\n",
      "|      theo| 13|            sci-fi|         2|       beginner|\n",
      "|      theo| 13|           fiction|         3|       beginner|\n",
      "|      mary| 25|historical fiction|         3|       beginner|\n",
      "|      mary| 25|           mystery|         7|       beginner|\n",
      "|      mary| 25|         dystopian|         8|       beginner|\n",
      "|      mary| 25|           romance|         8|       beginner|\n",
      "|      mary| 25|            satire|         4|       beginner|\n",
      "|      mary| 25|            sci-fi|        14|   intermediate|\n",
      "|      mary| 25|            memoir|        16|   intermediate|\n",
      "|      john| 50|         dystopian|         8|       beginner|\n",
      "|      john| 50|          thriller|        12|   intermediate|\n",
      "|      john| 50|        true crime|         3|       beginner|\n",
      "|      jane| 61|           romance|        20|   intermediate|\n",
      "+----------+---+------------------+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Identify the 'reading_fanatic' level for a given person and genre.\n",
    "#'BETWEEN ... AND ...' is inclusive on both ends.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *,\n",
    "        CASE WHEN books_read < 10 THEN 'beginner'\n",
    "             WHEN books_read BETWEEN 10 AND 30 THEN 'intermediate'\n",
    "             WHEN books_read > 30 THEN 'advanced'\n",
    "             ELSE 'N/A'\n",
    "             END AS reading_fanatic\n",
    "    FROM books_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebfa88d",
   "metadata": {},
   "source": [
    "## Window Functions\n",
    "While GROUP BY and HAVING are imperative for aggregations, there are times where window functions can be more useful. Window functions implement aggregations by partitioning (grouping) over columns that may be a subset of what is in the GROUP BY.\n",
    "- The syntax follows as AGG(column) OVER (PARTITION BY column1, column2,...) ORDER BY (column3, column4,...)\n",
    "- Only AGG(column) OVER (PARTITION BY (column1) is required at the minimum.\n",
    "- This can be used within a case expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "620b2a4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:28.339681Z",
     "start_time": "2025-05-15T19:53:27.833373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|first_name|reading_fanatic|\n",
      "+----------+---------------+\n",
      "|      alex|   intermediate|\n",
      "|      ally|   intermediate|\n",
      "|       bob|   intermediate|\n",
      "|       eli|       beginner|\n",
      "|       eve|       beginner|\n",
      "|      jane|       advanced|\n",
      "|      john|   intermediate|\n",
      "|      josh|   intermediate|\n",
      "|    martin|   intermediate|\n",
      "|      mary|       advanced|\n",
      "|       sue|   intermediate|\n",
      "|      theo|       beginner|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Identify the 'reading_fanatic' level for a given person and the total number of books they have read.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT DISTINCT first_name,\n",
    "        CASE WHEN SUM(books_read) OVER (PARTITION BY first_name) < 10 THEN 'beginner'\n",
    "             WHEN SUM(books_read) OVER (PARTITION BY first_name) BETWEEN 10 AND 30 THEN 'intermediate'\n",
    "             WHEN SUM(books_read) OVER (PARTITION BY first_name) > 30 THEN 'advanced'\n",
    "             ELSE 'N/A'\n",
    "             END AS reading_fanatic\n",
    "    FROM books_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5cc712",
   "metadata": {},
   "source": [
    "## User Defined Function\n",
    "We used the CASE expression to assign a reading_fanatic level, we can also do something similar using a UDF. Instead of assigning a reading_fanatic level, we'll create an age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2a593ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:29.011697Z",
     "start_time": "2025-05-15T19:53:28.343356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|age_group|\n",
      "+----------+---------+\n",
      "|       sue|    Adult|\n",
      "|       sue|    Adult|\n",
      "|       eli|    Youth|\n",
      "|       bob|    Adult|\n",
      "|       bob|    Adult|\n",
      "|       bob|    Adult|\n",
      "|       bob|    Adult|\n",
      "|      theo|    Youth|\n",
      "|      theo|    Youth|\n",
      "|      mary|    Youth|\n",
      "|      mary|    Youth|\n",
      "|      mary|    Youth|\n",
      "|      mary|    Youth|\n",
      "|      mary|    Youth|\n",
      "|      mary|    Youth|\n",
      "|      mary|    Youth|\n",
      "|      john|    Adult|\n",
      "|      john|    Adult|\n",
      "|      john|    Adult|\n",
      "|      jane|    Adult|\n",
      "+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create function age_group and determine if a reader is a Youth or an Adult.\n",
    "@functions.udf(returnType=types.StringType())\n",
    "def age_group(age):\n",
    "    return \"Adult\" if int(age) > 30 else \"Youth\"\n",
    "\n",
    "spark.udf.register(\"age_group\", age_group)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT first_name, age_group(age) AS age_group \n",
    "    FROM books_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8af6f2f",
   "metadata": {},
   "source": [
    "## Insert Statement\n",
    "In this native Spark table, we can insert new data using the INSERT statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db2d4d65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:29.225815Z",
     "start_time": "2025-05-15T19:53:29.013506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Insert data for a new reader.\n",
    "spark.sql(\"INSERT INTO books_table VALUES ('alice', 30, 'thriller', 10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f7dbf79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:29.584673Z",
     "start_time": "2025-05-15T19:53:29.228532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------+----------+\n",
      "|first_name|age|   genre|books_read|\n",
      "+----------+---+--------+----------+\n",
      "|     alice| 30|thriller|        10|\n",
      "+----------+---+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display the row that corresponds to the new reader Alice.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM books_table\n",
    "    WHERE first_name = 'alice' \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020030e",
   "metadata": {},
   "source": [
    "# Joining Tables\n",
    "Oftentimes one table alone may not have all the information we need. In our example, let's consider another table which provides for how much our readers purchased their books."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f71456",
   "metadata": {},
   "source": [
    "## Load Prices Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8e0580e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:30.018613Z",
     "start_time": "2025-05-15T19:53:29.595923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+-------------------+\n",
      "|first_name|    genre|price_per_book|num_books_purchased|\n",
      "+----------+---------+--------------+-------------------+\n",
      "|       sue|   sci-fi|           3.3|                  2|\n",
      "|       sue|   sci-fi|          4.75|                  6|\n",
      "|       sue|biography|          8.84|                  4|\n",
      "|       eli|  fantasy|          null|                  1|\n",
      "|       bob|  mystery|          5.73|                  6|\n",
      "+----------+---------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- price_per_book: string (nullable = true)\n",
      " |-- num_books_purchased: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load and display prices dataframe from the csv file.\n",
    "prices_df = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"prices.csv\")\n",
    "prices_df.show(5)\n",
    "\n",
    "#Display the schema.\n",
    "prices_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cc44513",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:30.618747Z",
     "start_time": "2025-05-15T19:53:30.028757Z"
    }
   },
   "outputs": [],
   "source": [
    "#If already created in current or previous SparkSession, drop table and remove the directory where the table existed.\n",
    "spark.sql(\"DROP TABLE IF EXISTS prices_table\")\n",
    "shutil.rmtree(\"/data/spark-warehouse/prices_table\", ignore_errors=True)\n",
    "spark.catalog.clearCache()\n",
    "#Write the prices_df to a table.\n",
    "prices_df.write.saveAsTable(\"prices_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5411c3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:30.710734Z",
     "start_time": "2025-05-15T19:53:30.629012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|         first_name|   string|   null|\n",
      "|              genre|   string|   null|\n",
      "|     price_per_book|   string|   null|\n",
      "|num_books_purchased|   string|   null|\n",
      "+-------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Describe the schema of the table.\n",
    "spark.sql(\"desc prices_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad78945",
   "metadata": {},
   "source": [
    "## Join books table with prices table\n",
    "We want to join the information regarding a person's name and age in the books table to the cost information in the prices table. For all our readers in the books table, we want to find the corresponding prices information. To achieve this we can perform a JOIN ON columns we expect to match between the two tables.\n",
    "\n",
    "Different types of Joins include:\n",
    "- LEFT\n",
    "- RIGHT\n",
    "- INNER\n",
    "- FULL\n",
    "- OUTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "357dbea0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:31.390033Z",
     "start_time": "2025-05-15T19:53:30.716247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------------------+----------+----------+------------------+--------------+-------------------+\n",
      "|first_name|age|             genre|books_read|first_name|             genre|price_per_book|num_books_purchased|\n",
      "+----------+---+------------------+----------+----------+------------------+--------------+-------------------+\n",
      "|      alex| 19|            sci-fi|        14|      alex|            sci-fi|            12|                  2|\n",
      "|      alex| 19|            sci-fi|        14|      alex|            sci-fi|          4.32|                  2|\n",
      "|      alex| 19|            sci-fi|        14|      alex|            sci-fi|          3.32|                 10|\n",
      "|     alice| 30|          thriller|        10|      null|              null|          null|               null|\n",
      "|      ally| 42|         biography|        14|      ally|         biography|          9.34|                 14|\n",
      "|      ally| 42|            memoir|         9|      ally|            memoir|          4.59|                  9|\n",
      "|       bob| 75|           mystery|        12|       bob|           mystery|          6.24|                  6|\n",
      "|       bob| 75|historical fiction|         4|       bob|historical fiction|           2.2|                  4|\n",
      "|       bob| 75|         biography|         5|       bob|         biography|           5.9|                  5|\n",
      "|       bob| 75|           mystery|        12|       bob|           mystery|          5.73|                  6|\n",
      "|       bob| 75|          thriller|         3|       bob|          thriller|            11|                  3|\n",
      "|       eli|  3|           fantasy|         1|       eli|           fantasy|          null|                  1|\n",
      "|       eve|  8|           fantasy|         7|       eve|           fantasy|          5.25|                  7|\n",
      "|      jane| 61|           romance|        20|      jane|           romance|          8.21|                 10|\n",
      "|      jane| 61|historical fiction|        12|      jane|historical fiction|          6.45|                 12|\n",
      "|      jane| 61|           romance|        20|      jane|           romance|          5.23|                 10|\n",
      "|      john| 50|          thriller|        12|      john|          thriller|           2.2|                 12|\n",
      "|      john| 50|         dystopian|         8|      john|         dystopian|          3.93|                  8|\n",
      "|      john| 50|        true crime|         3|      john|        true crime|          6.47|                  3|\n",
      "|      josh| 22|            satire|         3|      josh|            satire|          6.99|                  3|\n",
      "+----------+---+------------------+----------+----------+------------------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Perform a left join between books_table and prices_table on first_name and genre.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM books_table b\n",
    "    LEFT JOIN prices_table p\n",
    "    ON b.first_name = p.first_name and b.genre = p.genre\n",
    "    ORDER BY b.first_name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9649a22c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:31.817925Z",
     "start_time": "2025-05-15T19:53:31.396155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------+----------+----------+-----+--------------+-------------------+\n",
      "|first_name|age|   genre|books_read|first_name|genre|price_per_book|num_books_purchased|\n",
      "+----------+---+--------+----------+----------+-----+--------------+-------------------+\n",
      "|     alice| 30|thriller|        10|      null| null|          null|               null|\n",
      "+----------+---+--------+----------+----------+-----+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Identify which rows have missing information in the prices table.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM books_table b\n",
    "    LEFT JOIN prices_table p\n",
    "    ON b.first_name = p.first_name and b.genre = p.genre\n",
    "    WHERE p.first_name is null and p.genre is null and p.price_per_book is null and p.num_books_purchased is null\n",
    "    ORDER BY b.first_name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f7e79",
   "metadata": {},
   "source": [
    "## Row Number Window Function\n",
    "There can be times where we want each row to have a unique identifier based on a certain grouping - in this case, we can use the ROW_NUMBER() window function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66ef7ec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:32.675904Z",
     "start_time": "2025-05-15T19:53:31.823660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-------+--------------+-------------------+-------+\n",
      "|first_name|age|  genre|price_per_book|num_books_purchased|row_num|\n",
      "+----------+---+-------+--------------+-------------------+-------+\n",
      "|       bob| 75|mystery|          6.24|                  6|      1|\n",
      "|       bob| 75|mystery|          5.73|                  6|      2|\n",
      "|      mary| 25| memoir|          9.49|                  9|      1|\n",
      "|      mary| 25| memoir|          11.1|                  4|      2|\n",
      "|      mary| 25| memoir|          null|                  3|      3|\n",
      "|      mary| 25| sci-fi|          6.43|                  5|      1|\n",
      "|      mary| 25| sci-fi|          3.45|                  9|      2|\n",
      "+----------+---+-------+--------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Readers like Bob and Mary have purchased multiple books of a genre at different prices. We can assign a row number based on person, genre, and price.\n",
    "#Display rows for memoir and sci-fi genres for Mary and the mystery genre for Bob.\n",
    "#Assign row number such that the greatest price and number of books purchased is the top row of each group.\n",
    "spark.sql(\"\"\"\n",
    "SELECT *, ROW_NUMBER() OVER (PARTITION BY first_name, genre ORDER BY price_per_book DESC, num_books_purchased DESC) AS row_num\n",
    "FROM(\n",
    "    SELECT b.first_name, b.age, p.genre, p.price_per_book as price_per_book, p.num_books_purchased\n",
    "    FROM books_table b\n",
    "    LEFT JOIN prices_table p\n",
    "    ON b.first_name = p.first_name AND b.genre = p.genre\n",
    ")\n",
    "WHERE (first_name = 'bob' and genre = 'mystery') OR (first_name = 'mary' and genre IN ('memoir', 'sci-fi'))\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9358e",
   "metadata": {},
   "source": [
    "# More Complex Queries - Aggregations on Joins\n",
    "Now, we can use the various clauses and functions of Spark SQL to determine how much readers have purchased their books for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5d1fe0",
   "metadata": {},
   "source": [
    "## Finding the Total Price Paid per Genre & per Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdbc03df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:33.763642Z",
     "start_time": "2025-05-15T19:53:32.688890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------------------+--------------------+---------------------+\n",
      "|first_name|age|             genre|total_cost_per_genre|total_cost_per_person|\n",
      "+----------+---+------------------+--------------------+---------------------+\n",
      "|      alex| 19|            sci-fi|               65.84|                65.84|\n",
      "|      ally| 42|         biography|              130.76|               172.07|\n",
      "|       bob| 75|           mystery|               71.82|               143.12|\n",
      "|      jane| 61|historical fiction|               77.40|               211.80|\n",
      "|      jane| 61|           romance|              134.40|               211.80|\n",
      "|      josh| 22|          thriller|               65.68|                86.65|\n",
      "|      mary| 25|         dystopian|               71.44|               413.81|\n",
      "|      mary| 25|            memoir|              129.81|               413.81|\n",
      "|      mary| 25|           romance|               59.68|               413.81|\n",
      "|      mary| 25|            sci-fi|               63.20|               413.81|\n",
      "+----------+---+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display total price per genre per reader where the total cost per genre is > 50.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT DISTINCT first_name, age, genre, \n",
    "               SUM(CAST(total_cost_per_pricing AS DECIMAL(10,2))) OVER (PARTITION BY first_name, age, genre) AS total_cost_per_genre,\n",
    "               SUM(CAST(total_cost_per_pricing AS DECIMAL(10,2))) OVER (PARTITION BY first_name, age) AS total_cost_per_person\n",
    "        FROM(\n",
    "            SELECT b.first_name, b.age, p.genre, coalesce(p.price_per_book,0) as price_per_book, num_books_purchased,\n",
    "                   COALESCE(p.price_per_book,0) * num_books_purchased as total_cost_per_pricing\n",
    "            FROM books_table b\n",
    "            LEFT JOIN prices_table p\n",
    "            ON b.first_name = p.first_name AND b.genre = p.genre\n",
    "        ) \n",
    "    )\n",
    "    WHERE total_cost_per_genre > 50\n",
    "    ORDER BY first_name, genre\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625024a",
   "metadata": {},
   "source": [
    "## Calculate Cost for a Reader using Highest Cost per Genre\n",
    "We can use the ROW_NUMBER() window function and nested subqueries to identify which row has the highest cost per genre per reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b198ace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:34.422505Z",
     "start_time": "2025-05-15T19:53:33.769485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------------------+--------------+-------------------+-------+\n",
      "|first_name|age|             genre|price_per_book|num_books_purchased|row_num|\n",
      "+----------+---+------------------+--------------+-------------------+-------+\n",
      "|      alex| 19|            sci-fi|          4.32|                  2|      1|\n",
      "|      alex| 19|            sci-fi|          3.32|                 10|      2|\n",
      "|      alex| 19|            sci-fi|            12|                  2|      3|\n",
      "|       bob| 75|           mystery|          6.24|                  6|      1|\n",
      "|       bob| 75|           mystery|          5.73|                  6|      2|\n",
      "|      jane| 61|           romance|          8.21|                 10|      1|\n",
      "|      jane| 61|           romance|          5.23|                 10|      2|\n",
      "|    martin| 29|historical fiction|          2.39|                  3|      1|\n",
      "|    martin| 29|historical fiction|             0|                  7|      2|\n",
      "|      mary| 25|            memoir|          9.49|                  9|      1|\n",
      "|      mary| 25|            memoir|          11.1|                  4|      2|\n",
      "|      mary| 25|            memoir|             0|                  3|      3|\n",
      "|      mary| 25|            sci-fi|          6.43|                  5|      1|\n",
      "|      mary| 25|            sci-fi|          3.45|                  9|      2|\n",
      "|       sue| 32|            sci-fi|          4.75|                  6|      1|\n",
      "|       sue| 32|            sci-fi|           3.3|                  2|      2|\n",
      "|      theo| 13|           fiction|          6.24|                  2|      1|\n",
      "|      theo| 13|           fiction|             5|                  1|      2|\n",
      "+----------+---+------------------+--------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Assign row numbers based on name, age, and genre with highest price at the top irrespective of number of books purchased.\n",
    "#Display the rows where there are differing prices within the group of name, age, and genre.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT first_name, age, genre, price_per_book, num_books_purchased, row_num\n",
    "    FROM (\n",
    "        SELECT first_name, age, genre, price_per_book, num_books_purchased,\n",
    "              COUNT(*) OVER (PARTITION BY first_name, genre) as num_pricing_per_genre,\n",
    "              ROW_NUMBER() OVER (PARTITION BY first_name, genre ORDER BY price_per_book DESC) AS row_num\n",
    "        FROM(\n",
    "            SELECT b.first_name, b.age, p.genre, coalesce(p.price_per_book,0) as price_per_book, num_books_purchased\n",
    "            FROM books_table b\n",
    "            LEFT JOIN prices_table p\n",
    "            ON b.first_name = p.first_name AND b.genre = p.genre\n",
    "        ) \n",
    "    )\n",
    "    WHERE num_pricing_per_genre > 1 \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd976451",
   "metadata": {},
   "source": [
    "Perhaps a more efficient and readable to achieve this is using a WITH clause and the FIRST_VALUE() fucntion.. A WITH clause achieves the same as nested subqueries, and can be more helpful when we want to perform joins within subqueries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08749d07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:35.276714Z",
     "start_time": "2025-05-15T19:53:34.425904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------------------+--------------------+\n",
      "|first_name|age|             genre|total_cost_per_genre|\n",
      "+----------+---+------------------+--------------------+\n",
      "|      alex| 19|            sci-fi|               60.48|\n",
      "|       bob| 75|           mystery|               74.88|\n",
      "|      jane| 61|historical fiction|               77.40|\n",
      "|      josh| 22|          thriller|               65.68|\n",
      "|      mary| 25|         dystopian|               71.44|\n",
      "|      mary| 25|           romance|               59.68|\n",
      "|      mary| 25|            sci-fi|               90.02|\n",
      "+----------+---+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Use the highest price per genre per person and calculate the new total_cost_per_genre for each person.\n",
    "#Display total price per genre and per person where the total cost per genre is > 50.\n",
    "spark.sql(\"\"\"\n",
    "WITH combined_tables as (\n",
    "    SELECT b.first_name, b.age, p.genre, coalesce(p.price_per_book,0) as price_per_book, coalesce(p.num_books_purchased,0) as num_books_purchased\n",
    "    FROM books_table b\n",
    "    LEFT JOIN prices_table p\n",
    "    ON b.first_name = p.first_name AND b.genre = p.genre\n",
    "), aggregations as (\n",
    "    SELECT *,\n",
    "        first_name, age, genre, price_per_book, num_books_purchased,\n",
    "        SUM(num_books_purchased) OVER (PARTITION BY first_name, genre) as num_books_per_genre,\n",
    "        FIRST_VALUE(price_per_book) OVER (PARTITION BY first_name, genre ORDER BY price_per_book DESC) AS new_price_per_book   \n",
    "    FROM combined_tables\n",
    "), new_pricing as (\n",
    "    SELECT DISTINCT first_name, age, genre, CAST(num_books_per_genre * new_price_per_book AS DECIMAL(4,2)) as total_cost_per_genre\n",
    "    FROM aggregations\n",
    ")\n",
    "    SELECT * \n",
    "    FROM new_pricing \n",
    "    WHERE total_cost_per_genre > 50\n",
    "    ORDER BY first_name, genre\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b521c7f",
   "metadata": {},
   "source": [
    "# Create & Partition Table using SparkSQL\n",
    "\n",
    "We may want to save our results from a query or sequence of queries into a table that may be a new source in the future. Let's create a new table from our results above. We'll create the same table twice:\n",
    "\n",
    "1. Using the CREATE TABLE statement in the sql function without any partitions.\n",
    "2. Saving the query results to a DataFrame and using a DataFrame operation to write the table, and adding a partition on first_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18821c50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:35.359933Z",
     "start_time": "2025-05-15T19:53:35.281247Z"
    }
   },
   "outputs": [],
   "source": [
    "#If already created in current or previous SparkSession, drop table(s) and remove the directory(s) where the table(s) existed.\n",
    "spark.sql(\"DROP TABLE IF EXISTS cost\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS cost_partitioned\")\n",
    "shutil.rmtree(\"/data/spark-warehouse/cost\", ignore_errors=True)\n",
    "shutil.rmtree(\"/data/spark-warehouse/cost_partitioned\", ignore_errors=True)\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2af18495",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:35.935731Z",
     "start_time": "2025-05-15T19:53:35.363513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create table 'cost' which shows the accurate total cost for each reader by genre. \n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE cost AS(\n",
    "    SELECT DISTINCT first_name, age, genre, \n",
    "           SUM(CAST(total_cost_per_pricing AS DECIMAL(10,2))) OVER (PARTITION BY first_name, age, genre) AS total_cost_per_genre,\n",
    "           SUM(CAST(total_cost_per_pricing AS DECIMAL(10,2))) OVER (PARTITION BY first_name, age) AS total_cost_per_person\n",
    "    FROM(\n",
    "        SELECT b.first_name, b.age, p.genre, coalesce(p.price_per_book,0) as price_per_book, num_books_purchased,\n",
    "               coalesce(p.price_per_book,0) * num_books_purchased as total_cost_per_pricing\n",
    "        FROM books_table b\n",
    "        LEFT JOIN prices_table p\n",
    "        ON b.first_name = p.first_name AND b.genre = p.genre\n",
    "    ) \n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ba735e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:37.302769Z",
     "start_time": "2025-05-15T19:53:35.939317Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create table 'cost_partitioned' which shows the accurate total cost for each reader by genre. Overwrite the table to add a partition column.\n",
    "df = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT first_name, age, genre, \n",
    "           SUM(CAST(total_cost_per_pricing AS DECIMAL(10,2))) OVER (PARTITION BY first_name, age, genre) AS total_cost_per_genre,\n",
    "           SUM(CAST(total_cost_per_pricing AS DECIMAL(10,2))) OVER (PARTITION BY first_name, age) AS total_cost_per_person\n",
    "    FROM(\n",
    "        SELECT b.first_name, b.age, p.genre, coalesce(p.price_per_book,0) as price_per_book, num_books_purchased,\n",
    "               coalesce(p.price_per_book,0) * num_books_purchased as total_cost_per_pricing\n",
    "        FROM books_table b\n",
    "        LEFT JOIN prices_table p\n",
    "        ON b.first_name = p.first_name AND b.genre = p.genre\n",
    "    ) \n",
    "\"\"\")\n",
    "df.write.mode(\"overwrite\").partitionBy(\"first_name\").saveAsTable(\"cost_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "949f2b30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:37.465989Z",
     "start_time": "2025-05-15T19:53:37.307257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+-----------+\n",
      "|namespace|       tableName|isTemporary|\n",
      "+---------+----------------+-----------+\n",
      "|  default|     books_table|      false|\n",
      "|  default|            cost|      false|\n",
      "|  default|cost_partitioned|      false|\n",
      "|  default|    prices_table|      false|\n",
      "|         |      books_view|       true|\n",
      "+---------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display all tables and views.\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3283166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:37.525728Z",
     "start_time": "2025-05-15T19:53:37.469358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------+\n",
      "|            col_name|    data_type|comment|\n",
      "+--------------------+-------------+-------+\n",
      "|                 age|       string|   null|\n",
      "|               genre|       string|   null|\n",
      "|total_cost_per_genre|decimal(20,2)|   null|\n",
      "|total_cost_per_pe...|decimal(20,2)|   null|\n",
      "|          first_name|       string|   null|\n",
      "|# Partition Infor...|             |       |\n",
      "|          # col_name|    data_type|comment|\n",
      "|          first_name|       string|   null|\n",
      "+--------------------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display the schema for the new table, note that first_name is shown as the partition column.\n",
    "spark.sql(\"desc cost_partitioned\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3316d481",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:53:37.697837Z",
     "start_time": "2025-05-15T19:53:37.533669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+---------------------+----------+\n",
      "|age|             genre|total_cost_per_genre|total_cost_per_person|first_name|\n",
      "+---+------------------+--------------------+---------------------+----------+\n",
      "| 75|         biography|               29.50|               143.12|       bob|\n",
      "| 75|historical fiction|                8.80|               143.12|       bob|\n",
      "| 75|           mystery|               71.82|               143.12|       bob|\n",
      "| 75|          thriller|               33.00|               143.12|       bob|\n",
      "+---+------------------+--------------------+---------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Query from the new table.\n",
    "spark.sql(\"SELECT * FROM cost_partitioned WHERE first_name = 'bob'\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
